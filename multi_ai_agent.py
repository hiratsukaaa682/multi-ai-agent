import streamlit as st
import json
import os
import operator
import logging
import uuid
import asyncio
from dotenv import load_dotenv, find_dotenv
from typing import List, Annotated, TypedDict

from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.messages import HumanMessage, AIMessage, BaseMessage, ToolMessage, messages_to_dict, messages_from_dict
from langchain_core.utils.function_calling import convert_to_openai_function
from langchain_google_genai import ChatGoogleGenerativeAI
from langgraph.prebuilt import ToolNode
from langgraph.graph import StateGraph, START, END
from langgraph.checkpoint.memory import MemorySaver
from langchain_mcp_adapters.client import MultiServerMCPClient

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    filename='agent_conversation.log',
    filemode='a'
)
logger = logging.getLogger(__name__)

MODEL_PRICING_PER_MILLION_TOKENS = {
    "gemini-2.0-flash": {
        "input": 0.35,
        "output": 0.70
    },
    "default": {
        "input": 0.35,
        "output": 0.70
    }
}

def calculate_cost(usage_metadata: dict, model_name: str) -> dict:
    if not usage_metadata:
        return {"input": 0.0, "output": 0.0, "total": 0.0}
    pricing = MODEL_PRICING_PER_MILLION_TOKENS.get(model_name, MODEL_PRICING_PER_MILLION_TOKENS["default"])
    input_tokens = usage_metadata.get("prompt_token_count", 0)
    output_tokens = usage_metadata.get("candidates_token_count", 0)
    input_cost = (input_tokens / 1_000_000) * pricing["input"]
    output_cost = (output_tokens / 1_000_000) * pricing["output"]
    total_cost = input_cost + output_cost
    return {"input": input_cost, "output": output_cost, "total": total_cost}

def run_async_in_sync(coro):
    try:
        loop = asyncio.get_running_loop()
    except RuntimeError:
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
    return loop.run_until_complete(coro)

def sanitize_schema(item):
    if isinstance(item, dict):
        item.pop('additionalProperties', None)
        item.pop('$schema', None)
        if 'type' in item and isinstance(item['type'], list):
            non_null_types = [t for t in item['type'] if str(t).upper() != 'NULL']
            item['type'] = str(non_null_types[0]).upper() if non_null_types else None
        for key, value in item.items():
            item[key] = sanitize_schema(value)
    elif isinstance(item, list):
        return [sanitize_schema(i) for i in item]
    return item

_ = load_dotenv(find_dotenv())
google_api_key = os.getenv("GOOGLE_API_KEY")

CONVERSATION_HISTORY_DIR = "conversation_history"
os.makedirs(CONVERSATION_HISTORY_DIR, exist_ok=True)

def save_conversation(session_id: str, messages: List[BaseMessage]):
    """ä¼šè©±ã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã™ã‚‹"""
    if not session_id or not messages:
        return
    file_path = os.path.join(CONVERSATION_HISTORY_DIR, f"{session_id}.json")
    with open(file_path, "w", encoding="utf-8") as f:
        json.dump(messages_to_dict(messages), f, ensure_ascii=False, indent=2)

def load_conversation(session_id: str) -> List[BaseMessage]:
    """JSONãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ä¼šè©±ã‚’èª­ã¿è¾¼ã‚€"""
    file_path = os.path.join(CONVERSATION_HISTORY_DIR, f"{session_id}.json")
    if not os.path.exists(file_path):
        return []
    with open(file_path, "r", encoding="utf-8") as f:
        try:
            data = json.load(f)
            return messages_from_dict(data)
        except (json.JSONDecodeError, TypeError):
            return []

def list_conversations() -> List[dict]:
    """ä¿å­˜ã•ã‚Œã¦ã„ã‚‹ä¼šè©±ã®ä¸€è¦§ã‚’å–å¾—ã™ã‚‹"""
    conversations = []
    for filename in os.listdir(CONVERSATION_HISTORY_DIR):
        if filename.endswith(".json"):
            session_id = filename[:-5]
            file_path = os.path.join(CONVERSATION_HISTORY_DIR, filename)
            try:
                mtime = os.path.getmtime(file_path)
                messages = load_conversation(session_id)
                first_user_message = next((m.content for m in messages if isinstance(m, HumanMessage) and m.additional_kwargs.get("role") != "internal_instruction"), "æ–°ã—ã„ä¼šè©±")
                title = first_user_message[:40] + "..." if len(first_user_message) > 40 else first_user_message
                conversations.append({"id": session_id, "title": title, "mtime": mtime})
            except Exception:
                continue
    conversations.sort(key=lambda x: x["mtime"], reverse=True)
    return conversations

def delete_conversation(session_id: str):
    """ä¼šè©±ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤ã™ã‚‹"""
    file_path = os.path.join(CONVERSATION_HISTORY_DIR, f"{session_id}.json")
    if os.path.exists(file_path):
        os.remove(file_path)

class AgentState(TypedDict):
    messages: List[BaseMessage]
    next: str

def create_worker(llm: ChatGoogleGenerativeAI, tools: list, system_prompt: str):
    prompt = ChatPromptTemplate.from_messages([
        ("system", system_prompt),
        MessagesPlaceholder(variable_name="messages"),
    ])
    return prompt | llm.bind_tools(tools)

def create_supervisor(llm: ChatGoogleGenerativeAI, worker_names: List[str]):
    system_prompt = (
        "ã‚ãªãŸã¯AIãƒãƒ¼ãƒ ã®ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã§ã™ã€‚ã‚ãªãŸã®ä»•äº‹ã¯ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è¦æ±‚ã‚’é”æˆã™ã‚‹ãŸã‚ã«ã€éƒ¨ä¸‹ã§ã‚ã‚‹ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒãƒ¼ãƒ ã‚’ç›£ç£ã™ã‚‹ã“ã¨ã§ã™ã€‚\n"
        "ä¼šè©±ã®å±¥æ­´å…¨ä½“ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è¦æ±‚ã€ãƒ¯ãƒ¼ã‚«ãƒ¼ã®ã“ã‚Œã¾ã§ã®ä½œæ¥­çµæœãªã©ï¼‰ã‚’æ³¨æ„æ·±ãç¢ºèªã—ã¦ãã ã•ã„ã€‚\n\n"
        "ä»¥ä¸‹ã®æ‰‹é †ã§è¡Œå‹•ã—ã¦ãã ã•ã„:\n"
        "1. **ã‚¿ã‚¹ã‚¯ã®åˆ†æ**: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è¦æ±‚ã‚’é”æˆã™ã‚‹ãŸã‚ã«å¿…è¦ãªã‚¹ãƒ†ãƒƒãƒ—ã‚’è€ƒãˆã¾ã™ã€‚è¤‡æ•°ã®ãƒ¯ãƒ¼ã‚«ãƒ¼ã«ã‚ˆã‚‹é€£æºãŒå¿…è¦ãªå ´åˆã‚‚ã‚ã‚Šã¾ã™ã€‚ä¾‹ãˆã°ã€'Webã‚µãƒ¼ãƒ•ã‚¡ãƒ¼'ãŒåé›†ã—ãŸæƒ…å ±ã‚’'ãƒ•ã‚¡ã‚¤ãƒ«ã‚ªãƒšãƒ¬ãƒ¼ã‚¿ãƒ¼'ãŒãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãè¾¼ã‚€ã€ã¨ã„ã£ãŸé€£æºã§ã™ã€‚\n"
        "2. **æ¬¡ã®è¡Œå‹•ã®æ±ºå®š**: åˆ†æã«åŸºã¥ãã€æ¬¡ã«å–ã‚‹ã¹ãè¡Œå‹•ã‚’æ±ºå®šã—ã¾ã™ã€‚\n"
        "   - **ãƒ¯ãƒ¼ã‚«ãƒ¼ã¸ã®æŒ‡ç¤º**: ç‰¹å®šã®ã‚¿ã‚¹ã‚¯ã‚’ãƒ¯ãƒ¼ã‚«ãƒ¼ã«ä»»ã›ã‚‹å ´åˆã€ãã®ãƒ¯ãƒ¼ã‚«ãƒ¼ã®åå‰ã‚’`next`ã«æŒ‡å®šã—ã€å…·ä½“çš„ãªæŒ‡ç¤ºå†…å®¹ã‚’`content`ã«è¨˜è¿°ã—ã¾ã™ã€‚**é‡è¦ãªã®ã¯ã€ä»¥å‰ã®ãƒ¯ãƒ¼ã‚«ãƒ¼ã®å‡ºåŠ›çµæœã‚’ã€æ¬¡ã®ãƒ¯ãƒ¼ã‚«ãƒ¼ã¸ã®æŒ‡ç¤ºã«å«ã‚ã‚‹ã“ã¨ã§ã™ã€‚** ã“ã‚Œã«ã‚ˆã‚Šã€ãƒ¯ãƒ¼ã‚«ãƒ¼é–“ã§æƒ…å ±ã‚’å¼•ãç¶™ãã“ã¨ãŒã§ãã¾ã™ã€‚\n"
        "   - **ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¸ã®ç›´æ¥å›ç­”**: å…¨ã¦ã®ã‚¿ã‚¹ã‚¯ãŒå®Œäº†ã—ãŸå ´åˆã€ã¾ãŸã¯ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚’å¿…è¦ã¨ã—ãªã„å˜ç´”ãªå¿œç­”ã®å ´åˆã¯ã€`next`ã«'FINISH'ã‚’æŒ‡å®šã—ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¸ã®æœ€çµ‚çš„ãªå›ç­”ã‚’`content`ã«è¨˜è¿°ã—ã¾ã™ã€‚\n"
        "   - **å¤±æ•—ã‹ã‚‰ã®å›å¾©**: ãƒ¯ãƒ¼ã‚«ãƒ¼ãŒã‚¿ã‚¹ã‚¯ã«å¤±æ•—ã—ãŸå ´åˆã¯ã€ä¼šè©±å±¥æ­´ã‚’ç¢ºèªã—ã€æŒ‡ç¤ºå†…å®¹ã‚’ä¿®æ­£ã—ã¦å†è©¦è¡Œã™ã‚‹ã‹ã€åˆ¥ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’æ¤œè¨ã—ã¦ãã ã•ã„ã€‚\n\n"
        f"åˆ©ç”¨å¯èƒ½ãªãƒ¯ãƒ¼ã‚«ãƒ¼:\n{chr(10).join(f'- {name}' for name in worker_names)}"
    )
    output_schema = {
        "title": "supervisor_decision",
        "type": "object",
        "properties": {
            "next": {"type": "string", "description": f"æ¬¡ã«å®Ÿè¡Œã™ã‚‹ãƒ¯ãƒ¼ã‚«ãƒ¼åï¼ˆ{', '.join(worker_names)} ã¾ãŸã¯ FINISHï¼‰"},
            "content": {"type": "string", "description": "ãƒ¯ãƒ¼ã‚«ãƒ¼ã¸ã®æŒ‡ç¤ºå†…å®¹ã€ã¾ãŸã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¸ã®æœ€çµ‚å›ç­”"}
        },
        "required": ["next", "content"]
    }
    prompt = ChatPromptTemplate.from_messages([
        ("system", system_prompt),
        MessagesPlaceholder(variable_name="messages"),
    ])
    llm_with_tool = llm.bind_tools(tools=[output_schema], tool_choice="supervisor_decision")
    return prompt, llm_with_tool

@st.cache_resource
def initialize_graph():
    supervisor_model_name = "gemini-2.0-flash"
    supervisor_llm_instance = ChatGoogleGenerativeAI(model=supervisor_model_name, temperature=0.0, google_api_key=google_api_key)
    worker_model_name = "gemini-2.0-flash"
    worker_llm_instance = ChatGoogleGenerativeAI(model=worker_model_name, temperature=0.0, google_api_key=google_api_key)
    
    with open("mcp_config.json", "r") as f:
        mcp_config = json.load(f)
    mcp_client = MultiServerMCPClient(mcp_config["mcpServers"])
    
    tools = run_async_in_sync(mcp_client.get_tools())
    sanitized_tools = [sanitize_schema(convert_to_openai_function(t)) for t in tools]
    
    workers = {
        "Webã‚µãƒ¼ãƒ•ã‚¡ãƒ¼": create_worker(worker_llm_instance, sanitized_tools, "ã‚ãªãŸã¯Webæ¤œç´¢ã®å°‚é–€å®¶ã§ã™ã€‚web-searchãƒ„ãƒ¼ãƒ«ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚\nä¸ãˆã‚‰ã‚ŒãŸæŒ‡ç¤ºã‚’é”æˆã™ã‚‹ãŸã‚ã«é©åˆ‡ãªæ¤œç´¢ãƒ¯ãƒ¼ãƒ‰ã‚’è€ƒãˆã€å¿…è¦ãªæƒ…å ±ã‚’æ¤œç´¢ã—ã¦ãã ã•ã„ã€‚æ¤œç´¢çµæœã¯ã€æ¬¡ã®æ‹…å½“è€…ï¼ˆã¾ãŸã¯æœ€çµ‚çš„ãªå›ç­”è€…ï¼‰ãŒç†è§£ã—ã‚„ã™ã„ã‚ˆã†ã«ã€æ˜ç¢ºã‹ã¤è©³ç´°ã«å ±å‘Šã—ã¦ãã ã•ã„ã€‚"),
        "ãƒ•ã‚¡ã‚¤ãƒ«ã‚ªãƒšãƒ¬ãƒ¼ã‚¿ãƒ¼": create_worker(worker_llm_instance, sanitized_tools, "ã‚ãªãŸã¯ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ“ä½œã™ã‚‹å°‚é–€å®¶ã§ã™ã€‚file-systemãƒ„ãƒ¼ãƒ«ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚\nä¸ãˆã‚‰ã‚ŒãŸæŒ‡ç¤ºï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚„æ›¸ãè¾¼ã‚€å†…å®¹ãªã©ï¼‰ã«æ­£ç¢ºã«å¾“ã£ã¦ã€ãƒ•ã‚¡ã‚¤ãƒ«æ“ä½œã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚æ“ä½œãŒæˆåŠŸã—ãŸã‹ã€å¤±æ•—ã—ãŸã‹ã‚’æ˜ç¢ºã«å ±å‘Šã—ã¦ãã ã•ã„ã€‚"),
    }
    
    supervisor_prompt, supervisor_llm = create_supervisor(supervisor_llm_instance, list(workers.keys()))

    def supervisor_node(state: AgentState):
        logger.info("--- Supervisor Node ---")
        logger.info(f"Input State: {state['messages']}")
        chain = supervisor_prompt | supervisor_llm
        response_message = chain.invoke({"messages": state["messages"]})
        usage_metadata = response_message.response_metadata.get("usage_metadata", {})
        costs = calculate_cost(usage_metadata, supervisor_model_name)
        logger.info(f"Cost (Supervisor - {supervisor_model_name}): Input: ${costs['input']:.6f}, Output: ${costs['output']:.6f}, Total: ${costs['total']:.6f}")
        logger.info(f"Token Usage (Supervisor): {usage_metadata}")
        tool_call = response_message.tool_calls[0]
        supervisor_output = tool_call['args']
        logger.info(f"Output: {supervisor_output}")
        content = supervisor_output.get("content", "")
        next_action = supervisor_output.get("next", "FINISH")
        supervisor_comment_content = content if next_action == "FINISH" else f"ã€æŒ‡ç¤º: {next_action}ã¸ã€‘\n{content}"
        supervisor_comment = AIMessage(content=supervisor_comment_content, name="Supervisor")
        if next_action != "FINISH":
            instruction_for_worker = HumanMessage(content=content, additional_kwargs={"role": "internal_instruction"})
            return {"messages": state["messages"] + [supervisor_comment, instruction_for_worker], "next": next_action}
        else:
            return {"messages": state["messages"] + [supervisor_comment], "next": next_action}

    def worker_node(state: AgentState):
        worker_name = state["next"]
        logger.info(f"--- Worker Node: {worker_name} ---")
        messages_for_worker = state['messages']
        logger.info(f"Input Messages for Worker: {messages_for_worker}")
        worker = workers[worker_name]
        response = worker.invoke({"messages": messages_for_worker}, {"recursion_limit": 10})
        usage_metadata = response.response_metadata.get("usage_metadata", {})
        costs = calculate_cost(usage_metadata, worker_model_name)
        logger.info(f"Cost ({worker_name} - {worker_model_name}): Input: ${costs['input']:.6f}, Output: ${costs['output']:.6f}, Total: ${costs['total']:.6f}")
        logger.info(f"Token Usage ({worker_name}): {usage_metadata}")
        logger.info(f"Output: {response}")
        finish_reason = response.response_metadata.get('finish_reason', '')
        if finish_reason == 'MALFORMED_FUNCTION_CALL' or (not response.content and not hasattr(response, 'tool_calls')):
            error_message = f"ï¼ˆã‚·ã‚¹ãƒ†ãƒ ã‚¨ãƒ©ãƒ¼ï¼š{worker_name}ãŒã‚¿ã‚¹ã‚¯ã®å®Ÿè¡Œã«å¤±æ•—ã—ã¾ã—ãŸã€‚ç†ç”±: {finish_reason}ã€‚æŒ‡ç¤ºã‚’ä¿®æ­£ã—ã¦å†è©¦è¡Œã—ã¾ã™ã€‚ï¼‰"
            logger.error(f"Worker {worker_name} failed. Reason: {finish_reason}. Message: {response.response_metadata.get('finish_message', 'N/A')}")
            response = AIMessage(content=error_message, name=worker_name)
        response.name = worker_name
        return {"messages": state["messages"] + [response]}

    _tool_node = ToolNode(tools)

    async def custom_tool_node(state: AgentState):
        tool_results = await _tool_node.ainvoke(state)
        return {"messages": state["messages"] + tool_results["messages"]}

    def after_worker_router(state: AgentState):
        last_message = state["messages"][-1]
        if hasattr(last_message, "tool_calls") and last_message.tool_calls:
            return "tools"
        return "supervisor"

    def supervisor_router(state: AgentState):
        next_val = state.get("next")
        if not next_val or next_val == "FINISH":
            return END
        return next_val

    workflow = StateGraph(AgentState)
    workflow.add_node("supervisor", supervisor_node)
    workflow.add_node("tools", custom_tool_node)
    for name in workers:
        workflow.add_node(name, worker_node)
        workflow.add_conditional_edges(name, after_worker_router, {"tools": "tools", "supervisor": "supervisor"})
    workflow.add_edge("tools", "supervisor")
    workflow.add_conditional_edges("supervisor", supervisor_router, {**{name: name for name in workers}, END: END})
    workflow.add_edge(START, "supervisor")
    memory = MemorySaver()
    app = workflow.compile(checkpointer=memory)
    return app

st.set_page_config(page_title="Multi-Agent AI", page_icon="âœ¨", layout="wide")

st.markdown("""
<style>
    .stApp { background-color: #f0f2f6; }
    [data-testid="stChatMessage"] { background-color: white; border-radius: 0.75rem; box-shadow: 0 1px 3px rgba(0,0,0,0.05); margin-bottom: 1rem; }
    [data-testid="stSidebar"] { background-color: #ffffff; border-right: 1px solid #e6e6e6; }
    .stButton>button { border-radius: 0.5rem; }
</style>
""", unsafe_allow_html=True)

with st.sidebar:
    st.header("ğŸ¤– ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ«ãƒ‘ãƒãƒ«")
    st.markdown("---")
    if st.button("â• æ–°ã—ã„ä¼šè©±ã‚’é–‹å§‹", use_container_width=True, type="primary"):
        st.session_state.session_id = str(uuid.uuid4())
        st.session_state.messages = []
        st.rerun()

    st.markdown("---")
    st.header("ğŸ“œ ä¼šè©±å±¥æ­´")
    past_conversations = list_conversations()
    if not past_conversations:
        st.caption("ã¾ã ä¼šè©±å±¥æ­´ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")

    for conv in past_conversations:
        col1, col2 = st.columns([4, 1])
        with col1:
            if st.button(conv["title"], key=f"load_{conv['id']}", use_container_width=True):
                st.session_state.session_id = conv["id"]
                st.session_state.messages = load_conversation(conv["id"])
                st.rerun()
        with col2:
            if st.button("ğŸ—‘ï¸", key=f"delete_{conv['id']}", use_container_width=True, help="ã“ã®ä¼šè©±ã‚’å‰Šé™¤ã—ã¾ã™"):
                delete_conversation(conv["id"])
                if st.session_state.get("session_id") == conv["id"]:
                    st.session_state.session_id = str(uuid.uuid4())
                    st.session_state.messages = []
                st.rerun()

st.title("âœ¨ Multi-Agent AI")
st.caption("AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒãƒ¼ãƒ ãŒå”èª¿ã—ã¦ã€ã‚ãªãŸã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’å‡¦ç†ã—ã¾ã™ã€‚")

if not google_api_key:
    st.error("Google AI Studioã®APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚.envãƒ•ã‚¡ã‚¤ãƒ«ã« GOOGLE_API_KEY ã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚")
    st.stop()

try:
    graph = initialize_graph()
except Exception as e:
    st.error(f"ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®åˆæœŸåŒ–ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
    st.exception(e)
    st.stop()

if "session_id" not in st.session_state:
    st.session_state.session_id = str(uuid.uuid4())
    st.session_state.messages = []

def render_internal_message(msg: BaseMessage):
    avatar_map = {"Supervisor": "ğŸ¤–", "Webã‚µãƒ¼ãƒ•ã‚¡ãƒ¼": "ğŸŒ", "ãƒ•ã‚¡ã‚¤ãƒ«ã‚ªãƒšãƒ¬ãƒ¼ã‚¿ãƒ¼": "ğŸ“", "internal_instruction": "ğŸ“", "tool_call": "ğŸ› ï¸", "tool_result": "âœ…"}
    name, avatar = "System", "âš™ï¸"
    if hasattr(msg, 'name') and msg.name:
        name, avatar = msg.name, avatar_map.get(msg.name, "ğŸ•µï¸")
    elif isinstance(msg, HumanMessage) and msg.additional_kwargs.get("role") == "internal_instruction":
        name, avatar = "å†…éƒ¨æŒ‡ç¤º", avatar_map["internal_instruction"]
    elif isinstance(msg, AIMessage) and hasattr(msg, 'tool_calls') and msg.tool_calls:
        name, avatar = "ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—", avatar_map["tool_call"]
    elif isinstance(msg, ToolMessage):
        name, avatar = f"ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œçµæœ ({msg.name})", avatar_map["tool_result"]
    with st.chat_message(name, avatar=avatar):
        if isinstance(msg, AIMessage) and not msg.content and hasattr(msg, 'tool_calls') and msg.tool_calls:
            tool_call = msg.tool_calls[0]
            st.info(f"ãƒ„ãƒ¼ãƒ« `{tool_call['name']}` ã‚’å‘¼ã³å‡ºã—ã¾ã™ã€‚")
            st.code(json.dumps(tool_call['args'], indent=2, ensure_ascii=False), language="json")
        elif isinstance(msg, ToolMessage):
            try:
                content_dict = json.loads(msg.content)
                st.code(json.dumps(content_dict, indent=2, ensure_ascii=False), language="json")
            except (json.JSONDecodeError, TypeError):
                st.code(str(msg.content), language="text")
        else:
            st.markdown(msg.content)

turns = []
current_turn_messages = []
for msg in st.session_state.get("messages", []):
    is_real_user_message = isinstance(msg, HumanMessage) and msg.additional_kwargs.get("role") != "internal_instruction"
    if is_real_user_message and current_turn_messages:
        turns.append(current_turn_messages)
        current_turn_messages = []
    current_turn_messages.append(msg)
if current_turn_messages:
    turns.append(current_turn_messages)

for turn in turns:
    user_message, agent_steps = turn[0], turn[1:]
    with st.chat_message("user", avatar="ğŸ§‘â€ğŸ’»"):
        st.markdown(user_message.content)
    if agent_steps:
        final_answer = None
        internal_steps = []
        last_step = agent_steps[-1]
        if isinstance(last_step, AIMessage) and last_step.name == "Supervisor":
            final_answer = last_step
            internal_steps = agent_steps[:-1]
        else:
            internal_steps = agent_steps
        if internal_steps:
            with st.expander("ğŸ§  ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹ã‚’è¦‹ã‚‹"):
                for step in internal_steps:
                    render_internal_message(step)
        if final_answer:
            with st.chat_message("assistant", avatar="ğŸ¤–"):
                st.markdown(final_answer.content)

if prompt := st.chat_input("Webæ¤œç´¢ã‚„ãƒ•ã‚¡ã‚¤ãƒ«æ“ä½œãªã©ã€ä½•ã§ã‚‚èã„ã¦ãã ã•ã„..."):
    st.session_state.messages.append(HumanMessage(content=prompt))
    with st.chat_message("user", avatar="ğŸ§‘â€ğŸ’»"):
        st.markdown(prompt)

    with st.spinner("ğŸ§  AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒæ€è€ƒä¸­..."):
        try:
            config = {"configurable": {"thread_id": str(uuid.uuid4())}}
            input_messages = {"messages": st.session_state.messages}
            final_state = run_async_in_sync(graph.ainvoke(input_messages, config))
            st.session_state.messages = final_state["messages"]
            save_conversation(st.session_state.session_id, st.session_state.messages)
            st.rerun()
        except Exception as e:
            st.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            st.exception(e)
